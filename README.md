# Deep Learning & Computer Vision Portfolio

> **Advanced Deep Learning Engineer** | PyTorch | Computer Vision | NLP | Optimization
>
> Comprehensive portfolio demonstrating expertise in designing, training, and deploying neural network architectures. Features custom autograd engines, state-of-the-art CNN implementations, and advanced RNN sequence modeling.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

---

## üöÄ Professional Summary

Deep Learning Engineer with a strong foundation in mathematical optimization and neural architecture design. Proven ability to translate complex theoretical concepts into robust, production-ready code. Specialized in **Computer Vision (CNNs)** and **Sequence Modeling (LSTM, GRU)**. Proficient in the full ML lifecycle: from data pipeline construction to hyperparameter tuning and model evaluation.

---

## üõ†Ô∏è Technical Competencies

| Domain | Key Skills & Technologies |
|--------|---------------------------|
| **Deep Learning** | PyTorch, TensorFlow, Backpropagation, Gradient Descent, Autograd, Transfer Learning |
| **Computer Vision** | CNNs (ResNet, VGG), Image Classification, Data Augmentation, OpenCV |
| **Sequence Modeling** | RNN, LSTM, GRU, NLP, Time Series Forecasting, Attention Mechanisms |
| **Optimization** | SGD, Adam, RMSprop, Hyperparameter Tuning (Optuna), Batch Normalization, Dropout, Regularization |
| **Data Engineering** | Pandas, NumPy, Scikit-learn, Data Preprocessing, Feature Engineering, ETL Pipelines |
| **Tools & DevOps** | Git, Docker, AWS/GCP, Linux, Jupyter, VS Code, CI/CD for ML |

---

## üìö Deep Learning Implementations

### [HW0: Neural Network Primitives & Architecture Design](./ece046211_hw0_208848499_205717150.ipynb)

**Objective:** Engineer complete neural network architectures from first principles using only NumPy to master the mathematical foundations of deep learning.

**Key Implementations:**

- **Forward & Backward Propagation**: Implemented the full training loop manually, including chain rule derivation for gradients.
- **Activation Functions**: Coded ReLU, Sigmoid, and Tanh functions and their derivatives.
- **Matrix Calculus**: Optimized performance using vectorized operations for batch processing.

**Skills:** NumPy, Linear Algebra, Calculus, Algorithm Design.

---

### [HW1: Advanced Optimization Algorithms & Autograd Engine](./ece046211_hw1_208848499_205717150.ipynb)

**Objective:** Build a custom automatic differentiation engine and implement modern optimization algorithms to understand the internals of frameworks like PyTorch.

**Key Implementations:**

- **Custom Autograd Engine**: Developed a reverse-mode automatic differentiation system to compute gradients dynamically.
- **Optimizers**: Implemented **SGD with Momentum**, **RMSprop**, and **Adam** from scratch.
- **Analysis**: Conducted convergence analysis and visualized loss landscapes to compare optimizer performance.

**Skills:** PyTorch Autograd, Optimization Theory, Computational Graphs, Hyperparameter Tuning.

---

### [HW2: Convolutional Neural Networks (CNN) & Computer Vision](./ece046211_hw2_208848499_205717150.ipynb)

**Objective:** Design and train deep CNN architectures for image classification and structured data analysis, applying Computer Vision techniques to scientific data.

**Key Implementations:**

- **CNN Architecture**: Designed multi-layer convolutional networks with **Pooling**, **Batch Normalization**, and **Dropout**.
- **Transfer Learning**: Adapted architectures for the **MAGIC Gamma Telescope dataset** (19,020 samples).
- **Regularization**: Implemented strategies to prevent overfitting and improve generalization.

**Skills:** CNNs, Computer Vision, Feature Extraction, Model Evaluation, Data Augmentation.

---

### [HW3: Sequential Modeling with RNNs, LSTMs, & GRUs](./ece046211_hw3_208848499_205717150.ipynb)

**Objective:** Master sequence modeling by implementing Recurrent Neural Networks and addressing the vanishing gradient problem with advanced architectures.

**Key Implementations:**

- **RNN Variants**: Implemented **Vanilla RNN**, **LSTM (Long Short-Term Memory)**, and **GRU (Gated Recurrent Unit)** cells.
- **Sequence Processing**: Handled variable-length sequences using padding and packing techniques.
- **Gradient Stability**: Applied **Gradient Clipping** to mitigate exploding gradients in deep temporal networks.

**Skills:** RNN, LSTM, GRU, NLP, Time Series Analysis, Sequence-to-Sequence Models.

---

## üìä Repository Structure

```text
DL-Portfolio/
‚îú‚îÄ‚îÄ DL-assignments.md                          # Portfolio Overview
‚îú‚îÄ‚îÄ ece046211_hw0_208848499_205717150.ipynb    # HW0: Neural Network Fundamentals
‚îú‚îÄ‚îÄ ece046211_hw1_208848499_205717150.ipynb    # HW1: Optimization & Autograd
‚îú‚îÄ‚îÄ ece046211_hw2_208848499_205717150.ipynb    # HW2: CNNs & Computer Vision
‚îî‚îÄ‚îÄ ece046211_hw3_208848499_205717150.ipynb    # HW3: Sequential Models (RNN/LSTM)
```

---

## üéì Education & Certification

**Technion - Israel Institute of Technology**
*Course: ECE 046211 - Deep Learning*

- Rigorous curriculum covering the mathematical and practical aspects of modern deep learning.
- Focus on implementation from scratch to ensure deep theoretical understanding.

**Mathematical Foundations:**
Linear Algebra, Calculus, Probability & Statistics, Optimization Theory, Automatic Differentiation, Backpropagation, Gradient Descent Variants

**Machine Learning Principles:**
Supervised Learning, Classification, Regression, Model Evaluation, Loss Functions, Overfitting Prevention, Train/Validation/Test Splits, Data Preprocessing, Feature Engineering, Performance Metrics

**Software Engineering:**
Algorithm Implementation, Code Optimization, Documentation, Research Paper Writing, Reproducible Research, Version Control, Model Deployment

---

## üìà Key Highlights

‚ú® **Complete Deep Learning Stack**: From mathematical foundations (NumPy implementations) to advanced neural architectures

üìä **Comprehensive Experimentation**: Systematic hyperparameter tuning, ablation studies, and performance benchmarking

üèóÔ∏è **Production-Ready Code**: Clean, documented, reproducible implementations ready for deployment

üßÆ **Mathematical Rigor**: Strong foundation in optimization theory, linear algebra, and calculus underlying all implementations




